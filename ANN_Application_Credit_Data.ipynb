{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtYhvYG8-lsw"
      },
      "outputs": [],
      "source": [
        "# Ignore the warnings\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "from numpy import expand_dims\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import StandardScalar, MinMaxScaler\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam, SGD"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD DATA SET\n",
        "\n",
        "credit_data = pd.read_csv(\"\")\n",
        "credit_data.head()"
      ],
      "metadata": {
        "id": "vPDppflxFk1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credit_data['class'] = credit_data['class']-1\n",
        "credit_data.shape"
      ],
      "metadata": {
        "id": "FlLuW93WFwte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class distribution\n",
        "credit_data['class'].value_counts()/len(credit_data)"
      ],
      "metadata": {
        "id": "5ykCJ7t3GI1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credit_data.info()"
      ],
      "metadata": {
        "id": "S7qgMWOmGX0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Data Preprocessing - EDA"
      ],
      "metadata": {
        "id": "QCdNDeDcGhvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "credit_data.isnull().sum() # MIssing value"
      ],
      "metadata": {
        "id": "SaLdkb3-Ga0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1 Encoding Categorical Data"
      ],
      "metadata": {
        "id": "cifqAJS_G84g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = credit_data.select_dtypes(include=\"object\").columns.tolist()\n",
        "print(cat_cols)"
      ],
      "metadata": {
        "id": "HJ5HtPeeGuPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_data = credit_data[cat_cols]\n",
        "cat_data.head()"
      ],
      "metadata": {
        "id": "-5k4WvzuHVeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert categorical variable into dummy\n",
        "cat_df = pd.get_dummies(cat_data)\n",
        "print(cat_data.shape)\n",
        "print(cat_df.shape)"
      ],
      "metadata": {
        "id": "9Q9qgNuxHtiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credit_data_encoded = pd.concat([cat_df, credit_data.drop(cat_cols, axis=1)], axis=1)\n",
        "credit_data_encoded.shape"
      ],
      "metadata": {
        "id": "Dmc1RYO4H-QM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credit_data_encoded.head()"
      ],
      "metadata": {
        "id": "5mg-cZwIIMhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 Train Test Split"
      ],
      "metadata": {
        "id": "f5yu0P36IrpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to scale the continuous variables and combine for later processing\n",
        "df_x_train, df_x_test, df_y_train, df_y_test = train_test_split(credit_data_encoded.drop('class'),y,train_size=0.7, random_state=42)"
      ],
      "metadata": {
        "id": "ljfcdbtQIVWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_x_train.shape)"
      ],
      "metadata": {
        "id": "d0UtsgrjJa_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pd.Series(df_y_train).value_counts(normalize=True))\n",
        "print(pd.Series(df_y_train).value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "iXf8WFEEJ2nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to scale the continuous variables and combine for later processing\n",
        "df_x_train, df_x_test, df_y_train, df_y_test = train_test_split(credit_data_encoded.drop('class'),y,stratify = y, train_size=0.7, random_state=42)"
      ],
      "metadata": {
        "id": "iHT4_FX0KPTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3 Scaling Numerical Data"
      ],
      "metadata": {
        "id": "DHugJQmLKmSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the class column from credit data\n",
        "num_cols = credit_data.iloc[:, :-1].select_dtypes(include = np.number),columns.tolist()\n",
        "num_cols"
      ],
      "metadata": {
        "id": "ymKmKFL0KZ26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training numerical data\n",
        "train_num_data = df_x_train(num_cols)\n",
        "train_num_data.head()"
      ],
      "metadata": {
        "id": "IEgefrRAUil6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.4 train Data fit-transformation"
      ],
      "metadata": {
        "id": "-OPODhrMU2cM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "train_num_data_scaled = scale.fit_transformr.(train_num_data)\n",
        "train_num_df_scaled = pd.DataFrame(train_num_data_scaled, index = train_num_data.index, columns=num_cols)\n",
        "train_num_df_scaled.head()"
      ],
      "metadata": {
        "id": "eX9WnmvFU6O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.5 Test data Transformation"
      ],
      "metadata": {
        "id": "k45k9EgXVpHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_num_data_scaled = scaler.transform(num_data_test)\n",
        "test_num_df_scaled = pd.DataFrame(test_num_data_scaled, index=test_num_data.index, columns=num_cols)\n",
        "test_num_df_scaled.head()"
      ],
      "metadata": {
        "id": "ZhhRMT31Vt_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Train and Test dataset"
      ],
      "metadata": {
        "id": "4O5oVK8cWg1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df = pd.concat([train_num_df_scaled, df_x_train.drop(num_cols, axis=1)]), axis=1)\n",
        "X_test_df = pd.concat([test_num_df_scaled, df_x_test.drop(num_cols, axis=1)]), axis=1)\n",
        "y_train_df = df_y_train\n",
        "y_test_df= df_y_test"
      ],
      "metadata": {
        "id": "gNiyEf5iWjxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting data into array"
      ],
      "metadata": {
        "id": "CMpwPW6cXeM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(X_train_df)\n",
        "X_test = np.array(X_test_df)\n",
        "y_train = expand_dims(y_train_df, axis=1)\n",
        "y_test = expand_dims(y_test_df, axis=1)"
      ],
      "metadata": {
        "id": "QdePt7LkXaKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define & Compile keras Model"
      ],
      "metadata": {
        "id": "pt7qgqRPYY_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def DNN_Binary_classifier_KS(_input_dim, metrics):\n",
        "\n",
        "  # lAYER 1\n",
        "  model = Sequential()\n",
        "  model.add(Dense(10,input_shape=(_input_dim,)))\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  # Layer 2\n",
        "  model.add(Dense(10))\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  #Layer 3\n",
        "  model.add(Dense(1))\n",
        "  model.add(Activation('sigmoid'))\n",
        "\n",
        "  adam_opt = Adam(lr=0.01)\n",
        "\n",
        "  print(model.summary)\n",
        "\n",
        "  # Model compilation\n",
        "  model.compile(loss='binary_crossentropy', optimizer=adam_opt, metrices)\n",
        "\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "8f237-ciW6nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Fit Keras Model"
      ],
      "metadata": {
        "id": "5blgrBggbnhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "\n",
        "input_dim = 61\n",
        "training_epochs = 50\n",
        "batch_size = 32\n",
        "metrics = [\"accuracy\"]\n",
        "valid_set = (X_test, y_test)"
      ],
      "metadata": {
        "id": "NZWc-owhbf3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DNN_Binary_Classification_KS(61, metrics).summary()"
      ],
      "metadata": {
        "id": "VWRVPI6WbX9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "bin_class_model = DNN_Binary_Classification_KS(input_dim, metrics)\n",
        "\n",
        "# Fit the model\n",
        "history = bin_class_model.fit(X_train, y_train, validation_data = valid_set, batch_size = batch-size, epochs = training_epochs)"
      ],
      "metadata": {
        "id": "exRjoAgrcL9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Save model"
      ],
      "metadata": {
        "id": "woEJmZQcdsTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "\n",
        "def saveModel_path(model_dir=\"\")\n",
        "os.makedir(model_dir, exist_ok=True)\n",
        "fileName = time.strftime(\"Model_%Y_%m_%d_%S_.h5\")\n",
        "model_path = os.path.join(model_dir, fileName)\n",
        "print(f\"\\n{model_path}\")\n",
        "return model_path"
      ],
      "metadata": {
        "id": "ma60X_AbdEAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_path = model.save(saveModel_path())\n",
        "unique_path"
      ],
      "metadata": {
        "id": "tagPvbhNdyxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Predictions"
      ],
      "metadata": {
        "id": "pP1DmOfcd0aR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred = bin_class_model.oredict_classes(X_train)\n",
        "y_test_pred = bin_class_model.oredict_classes(X_test)"
      ],
      "metadata": {
        "id": "6onjJIuhd3D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training accuracy\n",
        "print(classification_report(y_train, y_train_pred))"
      ],
      "metadata": {
        "id": "8nQbfjX9ecSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test accuracy\n",
        "print(classification_report(y_test, y_test_pred))"
      ],
      "metadata": {
        "id": "u6zCBVqpew4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "QJQOR7t18O0O",
        "outputId": "13ecb587-901d-4dc7-eda5-46efd72da223"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-6e218b989568>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    git remote add origin https://github.com/tazdikhossain/ANN-Application-Credit-Data.ipynb.git\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}